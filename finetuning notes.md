# Notes about loading, inferring and fine-tuning LLM



## Huggingface ğŸ¤— 



### *classmethod* `from_pretrained`

```python
# sample1: load model in 8bit
model = AutoModelForCausalLM.from_pretrained(
    model_path, 
    load_in_8bit=True, 
    device_map='auto'
)

# sample2: load model in fp16
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    use_cache=not no_gradient_checkpointing,
    torch_dtype=torch.float16,
)
```

#### *parameters* pretrained_model_name_or_path 

* éœ€è¦åŠ è½½çš„æ¨¡å‹åç§°æˆ–è€…åŠ è½½æ¨¡å‹çš„æœ¬åœ°è·¯å¾„

#### *parameters* cache_dir

* æŒ‡å®šæ¨¡å‹ä¸‹è½½çš„ä½ç½®
* :exclamation: è‹¥ä½¿ç”¨Azure ML Studioæä¾›çš„æœºå™¨ï¼Œè®°å¾—ç¬¬ä¸€æ¬¡ä¸‹è½½å¤§æ¨¡å‹æ—¶ä¸€å®šè¦å°†è·¯å¾„æŒ‡æ˜åˆ°`â€/mnt/batch/tasks/shared/LS_root/mounts/clusters/xxxxx"`ä¸‹ï¼Œè¯¥è·¯å¾„ä¸‹æœ‰å……è¶³çš„å­˜å‚¨ç©ºé—´ã€‚é»˜è®¤çš„è·¯å¾„ä¸‹å­˜å‚¨ç©ºé—´åªæœ‰60Gï¼Œä¸è¶³ä»¥å­˜å‚¨å¤§æ¨¡å‹ã€‚
* å°†æ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°ä¹‹åï¼Œå¯ä»¥ç›´æ¥å°†`pretrained_model_name_or_path`æ”¹ä¸ºæœ¬åœ°å­˜å‚¨çš„è·¯å¾„ï¼Œä¸å¿…æ¯æ¬¡éƒ½æŒ‡å®š`cache_dir`

#### *parameters* device_map [(:link:)](https://huggingface.co/docs/accelerate/usage_guides/big_modeling#loading-weights)

* åœ¨from_pretrainedçš„æ—¶å€™ä¾¿å¯ä»¥å°†æ¨¡å‹åŠ è½½åˆ°æŒ‡å®šçš„GPUæˆ–CPUä¸­ï¼Œ è‹¥ä¸æŒ‡å®šï¼Œåˆ™é»˜è®¤åŠ è½½åˆ°CPUä¸­

* éœ€è¦æå‰å®‰è£…`accelerate`åŒ…ï¼ˆä½†æ— éœ€åœ¨ä»£ç ä¸­importï¼‰

* å¯é€‰çš„å‡ ç§å‚æ•°ä¸º"auto", "balanced", "balanced_low_0", "sequential", æˆ–è€…è‡ªå®šä¹‰ä¸€ä¸ª *dict* ä¹Ÿå¯

  * `auto` ä¸ `balanced` ç±»ä¼¼ï¼Œä¼šå°†**æ¨¡å‹å¹³å‡åˆ’åˆ†**åˆ°æ‰€æœ‰å¯ç”¨çš„GPUä¸Šï¼Œè¿™æ ·å¯ä»¥å°†batch_sizeè®¾å¤§ä¸€äº›ï¼Œæ³¨æ„å¦‚æœGPUä¸å¤Ÿï¼Œautoä¼šåŒæ—¶åˆ©ç”¨CPUå­˜å‚¨ä¸€éƒ¨åˆ†æ¨¡å‹å‚æ•°

    > The options `"auto"` and `"balanced"` produce the same results for now, but the behavior of `"auto"` might change in the future if we find a strategy that makes more sense, while `"balanced"` will stay stable.

  * `balanced_low_0`ï¼šä¼šåœ¨é™¤äº†ç¬¬ä¸€å¼ å¡ï¼ˆ"cuda:0"ï¼‰ä¹‹å¤–çš„æ‰€æœ‰GPUä¸Šå¹³å‡åˆ†é…æ¨¡å‹å‚æ•°ï¼Œè¿™æ ·å¯ä»¥åœ¨ç¬¬ä¸€å¼ å¡ä¸Šåšä¸€äº›é¢å¤–çš„æ“ä½œï¼Œä¾‹å¦‚å½“ä½¿ç”¨generateå‡½æ•°æ—¶å­˜æ”¾è¾“å‡ºæ•°æ®

  * `sequential`ï¼šå…ˆå°è¯•ç”¨ç¬¬ä¸€å¼ å¡ï¼Œå½“ç¬¬ä¸€å¼ å¡ç”¨å®Œæ—¶å¯ä»¥å†ç”¨ç¬¬äºŒå¼ å¡ï¼Œä»¥æ­¤ç±»æ¨

  * ä¸€ä¸ªè‡ªå®šä¹‰çš„ä¾‹å­ `device_map={'':Accelerator().process_index}`ï¼Œå°†æ¨¡å‹å…¨éƒ¨æ”¾å…¥å½“å‰processæ‰€åœ¨çš„GPUä¸­ï¼ˆprocess0å¯¹åº”çš„å°±æ˜¯GPU0ï¼‰

* æŸ¥çœ‹å½“å‰model çš„ device_mapï¼š`model.hf_device_map`

* å½“å‚æ•° load_in_8bit ä¸ºçœŸæ—¶ï¼Œå¿…é¡»æŒ‡å®šdevice_map

  > load the map into mixed-8bit quantized model not compiled for **CPU**!

* :exclamation:**LIMITATIONS**: 

  > The model parallelism used when your model is split on several GPUs is naive and not optimized, meaning that only one GPU works at a given time and the other sits idle.
  >
  * ç›®å‰device_mapåªèƒ½åšåˆ°æœ€ä¸ºåŸºç¡€çš„model parallelism (naive MP)ï¼Œæ²¡æœ‰pipelineï¼Œ å› æ­¤æ¯ä¸ªæ—¶åˆ»åªæœ‰ä¸€å¼ å¡åœ¨è¿è¡Œï¼Œæ•ˆç‡å¾ˆä½
   <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-gpipe-bubble.png"  />

#### *parameters* load_in_8bit[(:link:)](https://huggingface.co/docs/transformers/main/main_classes/quantization)

* ä»¥8bitç²¾åº¦åŠ è½½æ¨¡å‹

* éœ€è¦æå‰å®‰è£…`bitsandbytes`åŒ…ï¼ˆä½†æ— éœ€åœ¨ä»£ç ä¸­importï¼‰

* æŸ¥çœ‹å½“å‰modelæ‰€å çš„å­˜å‚¨ç©ºé—´ï¼š`model.get_memory_footprint()`

  * è¿”å›bytesï¼Œå¸¸ç”¨è¯­å¥

    ```python
    print(f"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB")
    ```

  * å¯ä»¥çœ‹åˆ°ä»¥8bitç²¾åº¦åŠ è½½çš„æ¨¡å‹æ‰€å çš„å†…å­˜éå¸¸å°‘ï¼Œstarcoderåªå äº†15939.61MB

* å¿…é¡»æŒ‡å®šdevice_map

* éœ€è¦ä¸*classmethod* `prepare_model_for_int8_training` æ­é…ä½¿ç”¨ï¼ˆè¯¥å‡½æ•°éœ€è¦å®‰è£…peftåŒ…ï¼‰ï¼Œè¯¥å‡½æ•°æœ‰ä»¥ä¸‹ä¸€äº›åŠŸèƒ½

  > - casts all the non `int8` modules to full precision (`fp32`) for stability
  > - adds a forward hook to the input embedding layer to calculate the gradients of the input hidden states
  > - enables gradient checkpointing for more memory-efficient training

  load_in_8bitç»å¸¸ä¸loraæ­é…ä½¿ç”¨ï¼Œloraä¼šå°†åŸå§‹modelçš„å‚æ•°å›ºå®šï¼Œåªä½¿ç”¨ä¸€ä¸ªä½ç§©çŸ©é˜µæ¥æ›´æ–°å‚æ•°ï¼Œå› æ­¤åœ¨ä¼ é€’æ¢¯åº¦æ—¶åªéœ€è¦**input embedding** å»å¼€å¯æ¢¯åº¦ä¼ æ’­ï¼Œä¸éœ€è¦æ•´ä¸ªæ¨¡å‹å»ä¼ æ’­æ¢¯åº¦ï¼ˆè¿™ä¹Ÿæ˜¯è¿™ä¸ªfunctionçš„ç¬¬äºŒç‚¹åŠŸèƒ½ï¼‰

#### *parameters* torch_dtype

* å½“è®¾ç½®`torch_dtype = torch.float16`æ—¶ï¼Œæ¨¡å‹ä»¥fp16ç²¾åº¦åŠ è½½ï¼Œå¦åˆ™é»˜è®¤ä»¥fp32ç²¾åº¦åŠ è½½

---



### *class* `TrainingAruguments`

```python
# sample: aruguments for fine-tuning starcoder
training_args = TrainingArguments(
    max_steps=arg.max_steps
    per_device_train_batch_size=args.batch_size,
    per_device_eval_batch_size=args.batch_size,
    learning_rate=args.learning_rate,
    lr_scheduler_type=args.lr_scheduler_type,
    warmup_steps=args.num_warmup_steps,
    gradient_accumulation_steps=args.gradient_accumulation_steps,
    gradient_checkpointing=not args.no_gradient_checkpointing,
    fp16=True,
    bf16=False,
    weight_decay=args.weight_decay,
    run_name="starcoder-finetuned",
    report_to="wandb",
    deepspeed=args.deepspeed,
)
```

#### *parameters* max_steps

* æ€»çš„è®­ç»ƒæ­¥æ•°ï¼Œä¸€æ—¦æŒ‡æ˜ä¹‹ånum_train_epochä¾¿å¤±æ•ˆäº†(override)
* æ¯æ¬¡stepç”¨ä¸€ä¸ªbatch_sizeçš„æ ·æœ¬è®­ç»ƒ

#### *parameters* gradient_checkpointing

* gradient checkpointing å¼€å¯åï¼Œåœ¨åå‘ä¼ æ’­æ—¶ä¼šé‡æ–°è®¡ç®—ç½‘ç»œä¸­é—´æ¿€æ´»å€¼

* å¼€å¯æ­¤åŠŸèƒ½å¯ä»¥èŠ‚çœè®¡ç®—å†…å­˜ï¼Œä½†ä¸æ­¤åŒæ—¶åå‘ä¼ æ’­çš„é€Ÿåº¦ä¼šæ›´æ…¢ï¼ˆä»¥æ—¶é—´æ¢ç©ºé—´ï¼‰

#### *parameters* fp16/bf16

* æ˜¯å¦ä½¿ç”¨fp16æˆ–è€…bf16æ··åˆç²¾åº¦è¿ç®—ï¼Œæ³¨æ„V100**ä¸æ”¯æŒ**bf16ç²¾åº¦

* å¼€å¯fp16ç²¾åº¦è®¡ç®—ä¹‹åï¼Œåœ¨å¤šå¡è®­ç»ƒæ—¶å¯èƒ½ä¼šå‡ºç°Runtime Error

  ```bash
  RuntimeError: expected scalar type Half but found Float
  ```

  * è§£å†³åŠæ³•å‚è€ƒæ­¤[:link:](https://stackoverflow.com/questions/75918140/getting-runtimeerror-expected-scalar-type-half-but-found-float-in-aws-p3-instan)ï¼šåœ¨åŸå§‹ä»£ç çš„åŸºç¡€ä¸Šåšä¿®æ”¹ï¼Œæ·»åŠ torch.autocast()åé”™è¯¯æ¶ˆå¤±ï¼Œæ¨æµ‹è¯¥é”™è¯¯å¯èƒ½ä¸V100ä¸Šçš„æ··åˆç²¾åº¦è¿ç®—æœ‰å…³

  * ```python
    with torch.autocast("cuda"): 
        trainer.train()
    ```

  * æ³¨ï¼šåŸåšä¸»è¯´åœ¨æ·»åŠ è¿™ä¸²ä»£ç ä¹‹åå¯èƒ½ä¼šå‡ºç°lossçš„å·¨å¤§æµ®åŠ¨æˆ–è€…lossä¸º0çš„æƒ…å†µï¼Œä½†ç›®å‰çš„è¿è¡Œç»“æœå¹¶æ²¡æœ‰å‡ºç°è¿™æ ·çš„é—®é¢˜

## DeepSpeed :rocket:



## æ‚é¡¹ :wrench:



### GPUæ˜¾å­˜

#### kernelçš„å¤§å°

å½“ä¸€ä¸ªæ¨¡å‹è¢«åŠ è½½åˆ°GPUä¸­æ—¶ï¼ŒGPUçš„å†…æ ¸(kernel)ä¹Ÿä¼šè¢«åŠ è½½ï¼Œå› æ­¤å³ä½¿æˆ‘ä»¬æ”¾äº†ä¸€ä¸ªå¾ˆå°çš„tensoråˆ°GPUä¸­ï¼Œä¹Ÿä¼šçœ‹åˆ°GPUå†…å­˜å·²ç»è¢«å æ®äº†1~2GBï¼Œè¿™å°±æ˜¯kernelçš„å¤§å°ï¼›åœ¨å®é™…æ˜¾å­˜è®¡ç®—ä¸­ä¹Ÿéœ€è¦è€ƒè™‘è¿™ä¸€éƒ¨åˆ†çš„å½±å“ã€‚
* V100çš„kernelå¤§å°å¤§è‡´ä¸º1300MB

#### è®­ç»ƒæ—¶çš„æ˜¾å­˜å ç”¨[(:link:)](https://huggingface.co/docs/transformers/perf_train_gpu_one#anatomy-of-models-memory)

åœ¨è®­ç»ƒçš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¼šæ³¨æ„åˆ°GPUçš„æ˜¾å­˜å ç”¨è¿œæ¯”å•çº¯åŠ è½½modelæ—¶è¦å¤§å¾—å¤šã€‚åœ¨è®­ç»ƒæ—¶ï¼Œæœ‰å¦‚ä¸‹è¿™äº›éƒ¨åˆ†å äº†GPUæ˜¾å­˜

* model weightsï¼šæ¨¡å‹çš„å‚æ•°ï¼Œæ³¨æ„å½“ä»¥fp16ç²¾åº¦ä¸fp32ç²¾åº¦çš„åŒºåˆ«

  > 4 bytes * number of parameters for fp32 training
  >
  > 6 bytes * number of parameters for mixed precision training (maintains a model in fp32 and **one in fp16 in memory**)

* optimizer states 

* gradients :æ— è®ºæ˜¯ä»¥ä½•ç§ç²¾åº¦è®­ç»ƒï¼Œæ¢¯åº¦ä¸€å¾‹ä»¥fp32ç²¾åº¦å­˜å‚¨

* forward activations saved for gradient computation : æ¿€æ´»å€¼å äº†è®­ç»ƒæ—¶çš„æ˜¾å­˜ä¸»è¦éƒ¨åˆ†ï¼Œä¸batch_sizeæˆæ­£æ¯”ï¼Œå­˜å‚¨æ¿€æ´»å€¼ç”¨äºæ¢¯åº¦çš„è®¡ç®—

* temporary buffers 

* functionality-specific memory

