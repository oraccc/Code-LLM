# Notes about loading, inferring and fine-tuning LLM



## Huggingface ğŸ¤— 



### *classmethod* `from_pretrained`

#### *parameters* device_map [(:link:)](https://huggingface.co/docs/accelerate/usage_guides/big_modeling#loading-weights)

* åœ¨from_pretrainedçš„æ—¶å€™ä¾¿å¯ä»¥å°†æ¨¡å‹åŠ è½½åˆ°æŒ‡å®šçš„GPUæˆ–CPUä¸­ï¼Œ è‹¥ä¸æŒ‡å®šï¼Œåˆ™é»˜è®¤åŠ è½½åˆ°CPUä¸­

* éœ€è¦æå‰å®‰è£…`accelerate`åŒ…ï¼ˆä½†æ— éœ€åœ¨ä»£ç ä¸­importï¼‰

* > You can let ğŸ¤— Accelerate handle the device map computation by setting `device_map` to one of the supported options (`"auto"`, `"balanced"`, `"balanced_low_0"`, `"sequential"`) or create one yourself, if you want more control over where each layer should go.

  å¯é€‰çš„å‡ ç§å‚æ•°ä¸º"auto", "balanced", "balanced_low_0", "sequential", æˆ–è€…è‡ªå®šä¹‰ä¸€ä¸ª *dict* ä¹Ÿå¯

  * `auto` ä¸ `balanced` ç±»ä¼¼ï¼Œä¼šå°†**æ¨¡å‹å¹³å‡åˆ’åˆ†**åˆ°æ‰€æœ‰å¯ç”¨çš„GPUä¸Šï¼Œè¿™æ ·å¯ä»¥å°†batch_sizeè®¾å¤§ä¸€äº›ï¼Œæ³¨æ„å¦‚æœGPUä¸å¤Ÿï¼Œautoä¼šåŒæ—¶åˆ©ç”¨CPUå­˜å‚¨ä¸€éƒ¨åˆ†æ¨¡å‹å‚æ•°

    > The options `"auto"` and `"balanced"` produce the same results for now, but the behavior of `"auto"` might change in the future if we find a strategy that makes more sense, while `"balanced"` will stay stable.

  * `balanced_low_0`ï¼šä¼šåœ¨é™¤äº†ç¬¬ä¸€å¼ å¡ï¼ˆ"cuda:0"ï¼‰ä¹‹å¤–çš„æ‰€æœ‰GPUä¸Šå¹³å‡åˆ†é…æ¨¡å‹å‚æ•°ï¼Œè¿™æ ·å¯ä»¥åœ¨ç¬¬ä¸€å¼ å¡ä¸Šåšä¸€äº›é¢å¤–çš„æ“ä½œï¼Œä¾‹å¦‚å½“ä½¿ç”¨generateå‡½æ•°æ—¶å­˜æ”¾è¾“å‡ºæ•°æ®

  * `sequential`ï¼šå…ˆå°è¯•ç”¨ç¬¬ä¸€å¼ å¡ï¼Œå½“ç¬¬ä¸€å¼ å¡ç”¨å®Œæ—¶å¯ä»¥å†ç”¨ç¬¬äºŒå¼ å¡ï¼Œä»¥æ­¤ç±»æ¨
  
  * ä¸€ä¸ªè‡ªå®šä¹‰çš„ä¾‹å­ `device_map={'':Accelerator().process_index}`ï¼Œå°†æ¨¡å‹å…¨éƒ¨æ”¾å…¥å½“å‰processæ‰€åœ¨çš„GPUä¸­ï¼ˆprocess0å¯¹åº”çš„å°±æ˜¯GPU0ï¼‰

* æŸ¥çœ‹å½“å‰model çš„ device_mapï¼š`model.hf_device_map`

* å½“å‚æ•° load_in_8bit ä¸ºçœŸæ—¶ï¼Œå¿…é¡»æŒ‡å®šdevice_map

  > load the map into mixed-8bit quantized model not compiled for **CPU**!

* :exclamation:**LIMITATIONS**: 

  > The model parallelism used when your model is split on several GPUs is naive and not optimized, meaning that only one GPU works at a given time and the other sits idle.

â€‹		ç›®å‰device_mapåªèƒ½åšåˆ°æœ€ä¸ºåŸºç¡€çš„model parallelism (naive MP)ï¼Œæ²¡æœ‰pipelineï¼Œ å› æ­¤æ¯ä¸ªæ—¶åˆ»åªæœ‰ä¸€å¼ å¡åœ¨è¿è¡Œï¼Œæ•ˆç‡å¾ˆä½



#### *parameters* load_in_8bit[(:link:)](https://huggingface.co/docs/transformers/main/main_classes/quantization)

* ä»¥8bitç²¾åº¦åŠ è½½æ¨¡å‹
* éœ€è¦æå‰å®‰è£…`bitsandbytes`åŒ…ï¼ˆä½†æ— éœ€åœ¨ä»£ç ä¸­importï¼‰
* æŸ¥çœ‹å½“å‰modelæ‰€å çš„å­˜å‚¨ç©ºé—´ï¼š`model.get_memory_footprint()`
  * å¯ä»¥çœ‹åˆ°ä»¥8bitç²¾åº¦åŠ è½½çš„æ¨¡å‹æ‰€å çš„å†…å­˜éå¸¸å°‘ï¼Œstarcoderåªå äº†15939.61MB

* å¿…é¡»æŒ‡å®šdevice_map

* éœ€è¦ä¸*classmethod* `prepare_model_for_int8_training` æ­é…ä½¿ç”¨ï¼ˆè¯¥å‡½æ•°éœ€è¦å®‰è£…peftåŒ…ï¼‰ï¼Œè¯¥å‡½æ•°æœ‰ä»¥ä¸‹ä¸€äº›åŠŸèƒ½

  > - casts all the non `int8` modules to full precision (`fp32`) for stability
  > - adds a forward hook to the input embedding layer to calculate the gradients of the input hidden states
  > - enables gradient checkpointing for more memory-efficient training

  load_in_8bitç»å¸¸ä¸loraæ­é…ä½¿ç”¨ï¼Œloraä¼šå°†åŸå§‹modelçš„å‚æ•°å›ºå®šï¼Œåªä½¿ç”¨ä¸€ä¸ªä½ç§©çŸ©é˜µæ¥æ›´æ–°å‚æ•°ï¼Œå› æ­¤åœ¨ä¼ é€’æ¢¯åº¦æ—¶åªéœ€è¦**input embedding** å»å¼€å¯æ¢¯åº¦ä¼ æ’­ï¼Œä¸éœ€è¦æ•´ä¸ªæ¨¡å‹å»ä¼ æ’­æ¢¯åº¦ï¼ˆè¿™ä¹Ÿæ˜¯è¿™ä¸ªfunctionçš„ç¬¬äºŒç‚¹åŠŸèƒ½ï¼‰



## DeepSpeed :rocket:



## æ‚é¡¹ :wrench:
