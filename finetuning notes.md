# Notes about loading, inferring and fine-tuning LLM



## Huggingface ğŸ¤— 



### *classmethod*   `from_pretrained`

```python
# sample1: load model in 8bit
model = AutoModelForCausalLM.from_pretrained(
    model_path, 
    load_in_8bit=True, 
    device_map='auto'
)

# sample2: load model in fp16
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    use_cache=not no_gradient_checkpointing,
    torch_dtype=torch.float16,
)
```

#### *parameters* pretrained_model_name_or_path 

* éœ€è¦åŠ è½½çš„æ¨¡å‹åç§°æˆ–è€…åŠ è½½æ¨¡å‹çš„æœ¬åœ°è·¯å¾„

#### *parameters* cache_dir

* æŒ‡å®šæ¨¡å‹ä¸‹è½½çš„ä½ç½®
* :exclamation: è‹¥ä½¿ç”¨Azure ML Studioæä¾›çš„æœºå™¨ï¼Œè®°å¾—ç¬¬ä¸€æ¬¡ä¸‹è½½å¤§æ¨¡å‹æ—¶ä¸€å®šè¦å°†è·¯å¾„æŒ‡æ˜åˆ°`â€/mnt/batch/tasks/shared/LS_root/mounts/clusters/xxxxx"`ä¸‹ï¼Œè¯¥è·¯å¾„ä¸‹æœ‰å……è¶³çš„å­˜å‚¨ç©ºé—´ã€‚é»˜è®¤çš„è·¯å¾„ä¸‹å­˜å‚¨ç©ºé—´åªæœ‰60Gï¼Œä¸è¶³ä»¥å­˜å‚¨å¤§æ¨¡å‹ã€‚
* å°†æ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°ä¹‹åï¼Œå¯ä»¥ç›´æ¥å°†`pretrained_model_name_or_path`æ”¹ä¸ºæœ¬åœ°å­˜å‚¨çš„è·¯å¾„ï¼Œä¸å¿…æ¯æ¬¡éƒ½æŒ‡å®š`cache_dir`
* å¦ä¸€ç§æ–¹å¼ï¼šå‚è€ƒ[å®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/transformers/installation?highlight=transformers_cache#cache-setup)ç›´æ¥åœ¨`bash.rc`é‡Œæ›´æ”¹ç¯å¢ƒå˜é‡`HUGGINGFACE_HUB_CACHE`

#### *parameters* device_map [(:link:)](https://huggingface.co/docs/accelerate/usage_guides/big_modeling#loading-weights)

* åœ¨from_pretrainedçš„æ—¶å€™ä¾¿å¯ä»¥å°†æ¨¡å‹åŠ è½½åˆ°æŒ‡å®šçš„GPUæˆ–CPUä¸­ï¼Œ è‹¥ä¸æŒ‡å®šï¼Œåˆ™é»˜è®¤åŠ è½½åˆ°CPUä¸­

* éœ€è¦æå‰å®‰è£…`accelerate`åŒ…ï¼ˆä½†æ— éœ€åœ¨ä»£ç ä¸­importï¼‰

* å¯é€‰çš„å‡ ç§å‚æ•°ä¸º"auto", "balanced", "balanced_low_0", "sequential", æˆ–è€…è‡ªå®šä¹‰ä¸€ä¸ª *dict* ä¹Ÿå¯

  * `auto` ä¸ `balanced` ç±»ä¼¼ï¼Œä¼šå°†**æ¨¡å‹å¹³å‡åˆ’åˆ†**åˆ°æ‰€æœ‰å¯ç”¨çš„GPUä¸Šï¼Œè¿™æ ·å¯ä»¥å°†batch_sizeè®¾å¤§ä¸€äº›ï¼Œæ³¨æ„å¦‚æœGPUä¸å¤Ÿï¼Œautoä¼šåŒæ—¶åˆ©ç”¨CPUå­˜å‚¨ä¸€éƒ¨åˆ†æ¨¡å‹å‚æ•°

    > The options `"auto"` and `"balanced"` produce the same results for now, but the behavior of `"auto"` might change in the future if we find a strategy that makes more sense, while `"balanced"` will stay stable.

  * `balanced_low_0`ï¼šä¼šåœ¨é™¤äº†ç¬¬ä¸€å¼ å¡ï¼ˆ"cuda:0"ï¼‰ä¹‹å¤–çš„æ‰€æœ‰GPUä¸Šå¹³å‡åˆ†é…æ¨¡å‹å‚æ•°ï¼Œè¿™æ ·å¯ä»¥åœ¨ç¬¬ä¸€å¼ å¡ä¸Šåšä¸€äº›é¢å¤–çš„æ“ä½œï¼Œä¾‹å¦‚å½“ä½¿ç”¨generateå‡½æ•°æ—¶å­˜æ”¾è¾“å‡ºæ•°æ®

  * `sequential`ï¼šå…ˆå°è¯•ç”¨ç¬¬ä¸€å¼ å¡ï¼Œå½“ç¬¬ä¸€å¼ å¡ç”¨å®Œæ—¶å¯ä»¥å†ç”¨ç¬¬äºŒå¼ å¡ï¼Œä»¥æ­¤ç±»æ¨

  * ä¸€ä¸ªè‡ªå®šä¹‰çš„ä¾‹å­ `device_map={'':Accelerator().process_index}`ï¼Œå°†æ¨¡å‹å…¨éƒ¨æ”¾å…¥å½“å‰processæ‰€åœ¨çš„GPUä¸­ï¼ˆprocess0å¯¹åº”çš„å°±æ˜¯GPU0ï¼‰

* æŸ¥çœ‹å½“å‰model çš„ device_mapï¼š`model.hf_device_map`

* å½“å‚æ•° load_in_8bit ä¸ºçœŸæ—¶ï¼Œå¿…é¡»æŒ‡å®šdevice_map

  > load the map into mixed-8bit quantized model not compiled for **CPU**!

* :exclamation:**LIMITATIONS**: 

  > The model parallelism used when your model is split on several GPUs is naive and not optimized, meaning that only one GPU works at a given time and the other sits idle.
  >
  * ç›®å‰device_mapåªèƒ½åšåˆ°æœ€ä¸ºåŸºç¡€çš„model parallelism (naive MP)ï¼Œæ²¡æœ‰pipelineï¼Œ å› æ­¤æ¯ä¸ªæ—¶åˆ»åªæœ‰ä¸€å¼ å¡åœ¨è¿è¡Œï¼Œæ•ˆç‡å¾ˆä½
   <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-gpipe-bubble.png"  />

#### *parameters* load_in_8bit [(:link:)](https://huggingface.co/docs/transformers/main/main_classes/quantization)

* ä»¥8bitç²¾åº¦åŠ è½½æ¨¡å‹

* éœ€è¦æå‰å®‰è£…`bitsandbytes`åŒ…ï¼ˆä½†æ— éœ€åœ¨ä»£ç ä¸­importï¼‰

* æŸ¥çœ‹å½“å‰modelæ‰€å çš„å­˜å‚¨ç©ºé—´ï¼š`model.get_memory_footprint()`

  * è¿”å›bytesï¼Œå¸¸ç”¨è¯­å¥

    ```python
    print(f"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB")
    ```

  * å¯ä»¥çœ‹åˆ°ä»¥8bitç²¾åº¦åŠ è½½çš„æ¨¡å‹æ‰€å çš„å†…å­˜éå¸¸å°‘ï¼Œstarcoderåªå äº†15939.61MB

* å¿…é¡»æŒ‡å®šdevice_map

* éœ€è¦ä¸*classmethod* `prepare_model_for_int8_training` æ­é…ä½¿ç”¨ï¼ˆè¯¥å‡½æ•°éœ€è¦å®‰è£…peftåŒ…ï¼‰ï¼Œè¯¥å‡½æ•°æœ‰ä»¥ä¸‹ä¸€äº›åŠŸèƒ½

  > - casts all the non `int8` modules to full precision (`fp32`) for stability
  > - adds a forward hook to the input embedding layer to calculate the gradients of the input hidden states
  > - enables gradient checkpointing for more memory-efficient training

  load_in_8bitç»å¸¸ä¸loraæ­é…ä½¿ç”¨ï¼Œloraä¼šå°†åŸå§‹modelçš„å‚æ•°å›ºå®šï¼Œåªä½¿ç”¨ä¸€ä¸ªä½ç§©çŸ©é˜µæ¥æ›´æ–°å‚æ•°ï¼Œå› æ­¤åœ¨ä¼ é€’æ¢¯åº¦æ—¶åªéœ€è¦**input embedding** å»å¼€å¯æ¢¯åº¦ä¼ æ’­ï¼Œä¸éœ€è¦æ•´ä¸ªæ¨¡å‹å»ä¼ æ’­æ¢¯åº¦ï¼ˆè¿™ä¹Ÿæ˜¯è¿™ä¸ªfunctionçš„ç¬¬äºŒç‚¹åŠŸèƒ½ï¼‰

#### *parameters* torch_dtype

* å½“è®¾ç½®`torch_dtype = torch.float16`æ—¶ï¼Œæ¨¡å‹ä»¥fp16ç²¾åº¦åŠ è½½ï¼Œå¦åˆ™é»˜è®¤ä»¥fp32ç²¾åº¦åŠ è½½

* :exclamation: å½“æ¨¡å‹ä»¥fp16ç²¾åº¦åŠ è½½çš„æ—¶å€™ï¼Œ**ä¸è¦**ä¸`prepare_model_for_int8_training`æ­é…ä½¿ç”¨ï¼Œè¯¥å‡½æ•°ä¼šå°†æ‰€æœ‰éint8ç²¾åº¦çš„æ•°å€¼å¼ºåˆ¶è½¬æ¢æˆfp32ç²¾åº¦ï¼Œè¿™æ ·å°±ä¸ç›®çš„èƒŒé“è€Œé©°äº†

* å½“ä»¥fp16ç²¾åº¦åŠ è½½æ¨¡å‹å¹¶ä½¿ç”¨LoRAè®­ç»ƒæ¨¡å‹æ—¶ï¼Œå¯èƒ½ä¼šå‡ºç°ä»¥ä¸‹Runtime Error

  ```bash
  Runtime Error: element 0 of tensors does not require grad and does not have a grad_fn
  ```

  è¿™æç¤ºæˆ‘ä»¬éœ€è¦**åœ¨input embeddingså¼€å¯æ¢¯åº¦ä¼ æ’­**ï¼ˆè¿™å…¶å®æ˜¯`prepare_model_for_int8_training`å‡½æ•°çš„ç¬¬äºŒç‚¹åŠŸèƒ½ï¼Œä½†ç”±äºæˆ‘ä»¬ä¸èƒ½ä½¿ç”¨è¿™ä¸ªå‡½æ•°ï¼Œå› æ­¤éœ€è¦æ‰‹åŠ¨å¼€å¯ï¼‰ï¼Œè¿˜é—®é¢˜å…·ä½“çš„è§£å†³æ–¹æ³•æ˜¯è°ƒç”¨æ–¹æ³•`model.enable_input_require_grads()`

  > Enables the gradients for the input embeddings. This is useful for fine-tuning adapter weights while keeping the model weights fixed.

* å› æ­¤ï¼Œå½“ä»¥fp16ç²¾åº¦åŠ è½½æ¨¡å‹å¹¶ä½¿ç”¨LoRAæ¥è®­ç»ƒæ—¶ï¼Œæ•´ä½“çš„æµç¨‹å¦‚ä¸‹

  ```python
  model.from_pretrained(path, torch_dtype=torch.float16)
  model.enable_input_require_grads()
  model.get_peft_model(model, lora_config)
  ```

  

---



### *class*  `TrainingAruguments`

```python
# sample: aruguments for fine-tuning starcoder
training_args = TrainingArguments(
    max_steps=arg.max_steps
    per_device_train_batch_size=args.batch_size,
    per_device_eval_batch_size=args.batch_size,
    learning_rate=args.learning_rate,
    lr_scheduler_type=args.lr_scheduler_type,
    warmup_steps=args.num_warmup_steps,
    gradient_accumulation_steps=args.gradient_accumulation_steps,
    gradient_checkpointing=not args.no_gradient_checkpointing,
    fp16=True,
    bf16=False,
    weight_decay=args.weight_decay,
    run_name="starcoder-finetuned",
    report_to="wandb",
    deepspeed=args.deepspeed,
)
```

#### *parameters* max_steps

* æ€»çš„è®­ç»ƒæ­¥æ•°ï¼Œä¸€æ—¦æŒ‡æ˜ä¹‹ånum_train_epochä¾¿å¤±æ•ˆäº†(override)
* æ¯æ¬¡stepç”¨ä¸€ä¸ªbatch_sizeçš„æ ·æœ¬è®­ç»ƒ

#### *parameters* gradient_checkpointing

* gradient checkpointing å¼€å¯åï¼Œåœ¨åå‘ä¼ æ’­æ—¶ä¼šé‡æ–°è®¡ç®—ç½‘ç»œä¸­é—´æ¿€æ´»å€¼

* å¼€å¯æ­¤åŠŸèƒ½å¯ä»¥èŠ‚çœè®¡ç®—å†…å­˜ï¼Œä½†ä¸æ­¤åŒæ—¶åå‘ä¼ æ’­çš„é€Ÿåº¦ä¼šæ›´æ…¢ï¼ˆä»¥æ—¶é—´æ¢ç©ºé—´ï¼‰

#### *parameters* fp16/bf16

* æ˜¯å¦ä½¿ç”¨fp16æˆ–è€…bf16æ··åˆç²¾åº¦è¿ç®—ï¼Œæ³¨æ„V100**ä¸æ”¯æŒ**bf16ç²¾åº¦

* å¼€å¯fp16ç²¾åº¦è®¡ç®—ä¹‹åï¼Œåœ¨å¤šå¡è®­ç»ƒæ—¶å¯èƒ½ä¼šå‡ºç°Runtime Error

  ```bash
  RuntimeError: expected scalar type Half but found Float
  ```

  * è§£å†³åŠæ³•å‚è€ƒæ­¤[:link:](https://stackoverflow.com/questions/75918140/getting-runtimeerror-expected-scalar-type-half-but-found-float-in-aws-p3-instan)ï¼šåœ¨åŸå§‹ä»£ç çš„åŸºç¡€ä¸Šåšä¿®æ”¹ï¼Œæ·»åŠ torch.autocast()åé”™è¯¯æ¶ˆå¤±ï¼Œæ¨æµ‹è¯¥é”™è¯¯å¯èƒ½ä¸V100ä¸Šçš„æ··åˆç²¾åº¦è¿ç®—æœ‰å…³

  * ```python
    with torch.autocast("cuda"): 
        trainer.train()
    ```

  * æ³¨ï¼šåŸåšä¸»è¯´åœ¨æ·»åŠ è¿™ä¸²ä»£ç ä¹‹åå¯èƒ½ä¼šå‡ºç°lossçš„å·¨å¤§æµ®åŠ¨æˆ–è€…lossä¸º0çš„æƒ…å†µï¼Œä½†ç›®å‰çš„è¿è¡Œç»“æœå¹¶æ²¡æœ‰å‡ºç°è¿™æ ·çš„é—®é¢˜

#### *parameters* deepspeed

* å¯ä»¥æ˜¯ä¸€ä¸ªdeepspeedçš„é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ä¼ å…¥deepspeedé…ç½® *dict*

---



## DeepSpeed :rocket:

### Trainer Deepspeed Integration [(:link:)](https://huggingface.co/docs/transformers/main/main_classes/deepspeed#deepspeed-integration)

ğŸ¤— Transformers é€šè¿‡Trainerå‹å¥½é›†æˆäº†deepspeedçš„æ ¸å¿ƒåŠŸèƒ½ï¼Œå› æ­¤ä¸éœ€è¦å¤§å¹…ä¿®æ”¹åŸå…ˆä»£ç ï¼Œåªéœ€è¦æä¾›deepspeedçš„é…ç½®æ–‡ä»¶å³å¯

> Integration of the core DeepSpeed features via [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer). This is an everything-done-for-you type of integration - just supply your custom config file or use our template and you have nothing else to do.

* å¯¹äº**Training**è¿‡ç¨‹ï¼Œdeepspeedæ”¯æŒZeRO stage1, 2, 3ä¸ZeRO-Infinity

* å¯¹äº**Inferrring**è¿‡ç¨‹ï¼Œ deepspeedæ”¯æŒZeRO stage3ä¸ZeRO-Infinity ï¼ˆå› ä¸ºstage2æ˜¯å¯¹æ¢¯åº¦åšåˆ’åˆ†ï¼Œå› æ­¤å¯¹inferenceæ²¡æœ‰ç”¨ï¼‰



#### è¿è¡ŒDeepSpeed

è‹¥è¦ä½¿ç”¨deepspeedï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¡Œè¿è¡Œ

```bash
deepspeed --num_gpus=8 your_program.py <normal cl args> --deepspeed ds_config.json
```

è‹¥ä¸æŒ‡æ˜num_gpusï¼Œåˆ™é»˜è®¤ä½¿ç”¨**å…¨éƒ¨çš„æ˜¾å¡**

:exclamation:ä¸æ”¯æŒåœ¨jupyter notebookä¸Šè¿è¡Œå¤šGPU



#### ä½¿ç”¨ds_reportè‡ªæŸ¥

åœ¨å®é™…ä½¿ç”¨deepspeedè¿è¡Œä»£ç ä¹‹å‰ï¼Œå¯ä»¥å…ˆæ£€æŸ¥ä¸€ä¸‹deepspeedçš„è¿è¡Œç¯å¢ƒï¼Œä½¿ç”¨å‘½ä»¤ `ds_report` å³å¯æŸ¥çœ‹ï¼Œéœ€è¦æ³¨æ„éœ€è¦ä¿æŒsystem installed cuda (nvcc version) ä¸ pytorch cuda versionçš„åŒ¹é…ï¼Œå¦åˆ™åœ¨è¿è¡Œæ—¶ä¼šæŠ¥é”™ã€‚

```bash
DeepSpeed general environment info:
torch install path ............... ['/anaconda/envs/starcoder/lib/python3.8/site-packages/torch']
torch version .................... 1.12.1
deepspeed install path ........... ['/anaconda/envs/starcoder/lib/python3.8/site-packages/deepspeed']
deepspeed info ................... 0.9.5, unknown, unknown
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
```

---



### Shared Configuration [(:link:)](https://huggingface.co/docs/transformers/main/main_classes/deepspeed#shared-configuration)

åœ¨å†™deepspeedçš„configurationæ—¶ï¼Œä¼šæ³¨æ„åˆ°æœ‰å¾ˆå¤šå‚æ•°ä¸Trainerçš„TrainingArugumentsæœ‰é‡å¤çš„éƒ¨åˆ†ï¼Œä¾‹å¦‚å­¦ä¹ ç‡ï¼Œä¼˜åŒ–å™¨å‚æ•°ç­‰ç­‰ï¼Œä¸¤è€…å¾ˆå®¹æ˜“æ··æ·†ï¼Œä½†è‹¥æ˜¯å¿½ç•¥æ‰è¿™äº›å‚æ•°ä¸å†™ï¼Œåè€Œæœ‰æ—¶ä¼šå¼•èµ·ç¨‹åºçš„æŠ¥é”™ï¼Œæç¤ºdeepspeed configç¼ºå°‘å‚æ•°ã€‚å› æ­¤å®˜æ–¹è¾ƒä¸ºæ¨èï¼ˆå¹¶ä¸”å®è·µèµ·æ¥æ²¡æœ‰å‡ºé”™ï¼‰çš„ä¸€ä¸ªåšæ³•ä¾¿æ˜¯ä½¿ç”¨å‚æ•°`â€œautoâ€`ï¼Œå¹¶å°†é…ç½®æ–‡ä»¶ä½œä¸ºå‚æ•°ä¼ é€’ç»™TrainingArugumentsï¼Œè¿™æ ·deepspeedå¯ä»¥è‡ªåŠ¨è¯»å–Trainerçš„å‚æ•°è®¾å®šï¼Œæˆ–è€…è®©Traineræ ¹æ®å®é™…æƒ…å†µè‡ªåŠ¨è®¾ç½®å‚æ•°ã€‚

å½“ç„¶ä¹Ÿå¯ä»¥è‡ªå·±è®¾ç½®è¿™äº›valueï¼Œä½†æ˜¯å¿…é¡»ç¡®ä¿ä¸Trainerä¸€è‡´ï¼Œä¸ç„¶å¯èƒ½ä¼šé€ æˆä¸å¯é¢„çŸ¥çš„é”™è¯¯ã€‚

> Note: currently DeepSpeed doesnâ€™t validate parameter names, so if you misspell any, itâ€™ll use the default setting for the parameter that got misspelled. You can watch the DeepSpeed engine start up log messages to see what values it is going to use.

ç›®å‰deepspeed configä¸æ”¯æŒæ‹¼å†™æ£€æŸ¥ï¼Œå› æ­¤ç”¨æˆ·éœ€è¦è‡ªå·±æ£€æŸ¥æ‹¼å†™ã€‚

ä¸€ä¸ªç®€å•çš„é…ç½®ä¾‹å­ï¼ˆéå®Œæ•´ç‰ˆï¼‰ï¼Œæ³¨æ„å…¶ä¸­ `train_batch_size` ä¸ `train_micro_batch_size_per_gpu` ä¸¤ä¸ªå‚æ•°å¿…é¡»æŒ‡å®šä¸€ä¸ª

```json
{
    "optimizer": {
      "type": "AdamW",
      "params": {
        "lr": "auto",
        "betas": "auto",
        "eps": "auto",
        "weight_decay": "auto"
      }
    },
    "scheduler": {
      "type": "WarmupLR",
      "params": {
        "warmup_min_lr": "auto",
        "warmup_max_lr": "auto",
        "warmup_num_steps": "auto"
      }
    },
    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto"
  }
```

### ZeRO é…ç½®

ZeROæœ‰å…³çš„é…ç½®æ˜¯æ•´ä¸ªds_configæ–‡ä»¶ä¸­æœ€ä¸ºé‡è¦çš„éƒ¨åˆ†

ZeROçš„å®ç°æ–¹æ³•æ˜¯æŠŠå‚æ•°å ç”¨åˆ†æˆä¸‰ç§ç±»å‹ã€‚å°†è¿™äº›ç±»å‹çš„å‚æ•°åˆ’åˆ†ï¼š

- `optimizer states`ï¼šå³ä¼˜åŒ–å™¨çš„å‚æ•°çŠ¶æ€ã€‚ä¾‹å¦‚Adamçš„åŠ¨é‡å‚æ•°ã€‚
- `gradients`ï¼šæ¢¯åº¦ç¼“å­˜ï¼Œå¯¹åº”äºoptimizerã€‚
- `parameters`ï¼šæ¨¡å‹å‚æ•°ã€‚

DeepSpeedçš„ZeRO configæ–‡ä»¶ä¹Ÿä¾æ®å¯ä»¥åˆ†ä¸ºå¦‚ä¸‹å‡ ç±»ï¼š

* ZeRO Stage 1: åˆ’åˆ†optimizer statesã€‚ä¼˜åŒ–å™¨å‚æ•°è¢«åˆ’åˆ†åˆ°å¤šä¸ªmemoryä¸Šï¼Œæ¯ä¸ªmomoeyä¸Šçš„è¿›ç¨‹åªè´Ÿè´£æ›´æ–°å®ƒè‡ªå·±é‚£éƒ¨åˆ†å‚æ•°
* ZeRO Stage 2: åˆ’åˆ†gradientã€‚æ¯ä¸ªmemoryåªä¿ç•™å®ƒåˆ†é…åˆ°çš„optimizer stateæ‰€å¯¹åº”çš„æ¢¯åº¦ã€‚
* ZeRO Stage 3: åˆ’åˆ†æ¨¡å‹å‚æ•°ã€‚ZeRO-3ä¼šåœ¨forwardå’Œbackwardçš„æ—¶å€™ï¼Œè‡ªåŠ¨å°†æ¨¡å‹å‚æ•°åˆ†é…åˆ°å¤šä¸ªmemoryã€‚

åœ¨å®é™…æƒ…å†µä¸­ï¼Œstage2ä¸3æ›´ä¸ºå®ç”¨



#### ZeRO Stage 2 é…ç½® [(:link:)](https://huggingface.co/docs/transformers/main/main_classes/deepspeed#zero2-config)

å¸¸ç”¨çš„stage 2é…ç½®æ˜¯

```json
{
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {
      "device": "cpu",
      "pin_memory": true
    },
    "allgather_partitions": true,
    "allgather_bucket_size": 1e8,
    "overlap_comm": true,
    "reduce_scatter": true,
    "reduce_bucket_size": 1e8,
    "contiguous_gradients": true
  }
}
```

ä¸€äº›é‡è¦çš„å‚æ•°è¯´æ˜ï¼Œè¿™äº›å‚æ•°å¾€å¾€æ˜¯é…ç½®æ–‡ä»¶ä¸­æœ€é‡è¦çš„éƒ¨åˆ†ï¼Œä¹Ÿæ˜¯åœ¨æ”¹å–„æ€§èƒ½æ˜¯éœ€è¦åå¤è°ƒæ•´çš„å‚æ•°ï¼Œè€Œé…ç½®æ–‡ä»¶ä¸­çš„å…¶ä½™å‚æ•°å®˜æ–¹å¼ºçƒˆæ¨èç›´æ¥è®¾ä¸º`"auto"`ï¼š

* **offload_optimizer**ï¼šå°†optimizerçŠ¶æ€å¸è½½åˆ°CPUæˆ–è€…NVMeä¸Šï¼Œ å¹¶å°†optimizerçš„è®¡ç®—å¸è½½åˆ°cpuä¸Šï¼Œå¯ç”¨è¿™ä¸ªåŠŸèƒ½å¯ä»¥é‡Šæ”¾GPUçš„å†…å­˜ï¼Œå…è®¸è®­ç»ƒæ›´å¤§çš„æ¨¡å‹æˆ–è€…ä½¿ç”¨æ›´å¤§çš„batch_size

  * å¯é€‰å‚æ•°æœ‰`"cpu"`,  `"nvme"`, `"none"`ï¼ˆä¸å¼€å¯ï¼‰
  * stage2ä¸3å‡æ”¯æŒ

* **allgather_partitions**ï¼šåœ¨æ¯ä¸ªæ­¥éª¤ç»“æŸæ—¶ä»æ‰€æœ‰GPUä¸­æ”¶é›†æ›´æ–°åçš„å‚æ•°

* **allgather_bucket_size**ï¼šä¸€æ¬¡æ€§æ”¶é›†çš„å…ƒç´ æ•°ç›®ï¼Œé™ä½è¯¥å€¼å¯ä»¥é™ä½æ‰€å çš„GPUæ˜¾å­˜ï¼Œä½†ä¼šå¢åŠ é€šè®¯å¼€é”€

* **overlap_comm**ï¼šå°è¯•å°†æ¢¯åº¦å‡å°‘ä¸åå‘è®¡ç®—é‡å ã€‚è‹¥è®¾ä¸ºçœŸï¼Œå¯ä»¥ä»¥å¢åŠ GPU RAMä¸ºä»£ä»·æ¥é™ä½å»¶è¿Ÿ

  * > `overlap_comm` uses 4.5x the `allgather_bucket_size` and `reduce_bucket_size` values. So if they are set to 5e8, this requires a 9GB footprint (`5e8 x 2Bytes x 2 x 4.5`). Therefore, if you have a GPU with 8GB or less RAM, to avoid getting OOM-errors you will need to reduce those parameters to about `2e8`, which would require 3.6GB. You will want to do the same on larger capacity GPU as well, if youâ€™re starting to hit OOM.

  * é€šè¿‡é™ä½`allgather_bucket_size`ä¸`reduce_bucket_size`ï¼Œå¯ä»¥é€šè¿‡ç‰ºç‰²é€šè®¯æ—¶é—´æ¥æ¢å–æ›´å¤šGPUæ˜¾å­˜ï¼Œè¿™æ ·ä¹Ÿå¯ä»¥è®¾ç½®æ›´å¤§çš„batch_sizeï¼Œ**batch_sizeå¤§å°**ä¸**è®­ç»ƒé€Ÿåº¦**æ˜¯ä¸€ä¸ªéœ€è¦æƒè¡¡çš„è¦ç´ 

* 



## æ‚é¡¹ :wrench:



### GPUæ˜¾å­˜

#### kernelçš„å¤§å°

å½“ä¸€ä¸ªæ¨¡å‹è¢«åŠ è½½åˆ°GPUä¸­æ—¶ï¼ŒGPUçš„å†…æ ¸(kernel)ä¹Ÿä¼šè¢«åŠ è½½ï¼Œå› æ­¤å³ä½¿æˆ‘ä»¬æ”¾äº†ä¸€ä¸ªå¾ˆå°çš„tensoråˆ°GPUä¸­ï¼Œä¹Ÿä¼šçœ‹åˆ°GPUå†…å­˜å·²ç»è¢«å æ®äº†1~2GBï¼Œè¿™å°±æ˜¯kernelçš„å¤§å°ï¼›åœ¨å®é™…æ˜¾å­˜è®¡ç®—ä¸­ä¹Ÿéœ€è¦è€ƒè™‘è¿™ä¸€éƒ¨åˆ†çš„å½±å“ã€‚
* V100çš„kernelå¤§å°å¤§è‡´ä¸º1300MB

#### è®­ç»ƒæ—¶çš„æ˜¾å­˜å ç”¨[(:link:)](https://huggingface.co/docs/transformers/perf_train_gpu_one#anatomy-of-models-memory)

åœ¨è®­ç»ƒçš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¼šæ³¨æ„åˆ°GPUçš„æ˜¾å­˜å ç”¨è¿œæ¯”å•çº¯åŠ è½½modelæ—¶è¦å¤§å¾—å¤šã€‚åœ¨è®­ç»ƒæ—¶ï¼Œæœ‰å¦‚ä¸‹è¿™äº›éƒ¨åˆ†å äº†GPUæ˜¾å­˜

* model weightsï¼šæ¨¡å‹çš„å‚æ•°ï¼Œæ³¨æ„å½“ä»¥fp16ç²¾åº¦ä¸fp32ç²¾åº¦çš„åŒºåˆ«

  > 4 bytes * number of parameters for fp32 training
  >
  > 6 bytes * number of parameters for mixed precision training (maintains a model in fp32 and **one in fp16 in memory**)

* optimizer states 

* gradients :æ— è®ºæ˜¯ä»¥ä½•ç§ç²¾åº¦è®­ç»ƒï¼Œæ¢¯åº¦ä¸€å¾‹ä»¥fp32ç²¾åº¦å­˜å‚¨

* forward activations saved for gradient computation : æ¿€æ´»å€¼å äº†è®­ç»ƒæ—¶çš„æ˜¾å­˜ä¸»è¦éƒ¨åˆ†ï¼Œä¸batch_sizeæˆæ­£æ¯”ï¼Œå­˜å‚¨æ¿€æ´»å€¼ç”¨äºæ¢¯åº¦çš„è®¡ç®—

* temporary buffers 

* functionality-specific memory

