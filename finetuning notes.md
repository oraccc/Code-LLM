# Notes about loading, inferring and fine-tuning LLM



## *class* AutoModelForCausalLM

### *classmethod* `from_pretrained`

#### *parameters* device_map

* åœ¨from_pretrainedçš„æ—¶å€™ä¾¿å¯ä»¥å°†æ¨¡å‹åŠ è½½åˆ°æŒ‡å®šçš„GPUæˆ–CPUä¸­

* éœ€è¦æå‰å®‰è£…accelerateåŒ…ï¼ˆä½†æ— éœ€åœ¨ä»£ç ä¸­importï¼‰

* > You can let ğŸ¤— Accelerate handle the device map computation by setting `device_map` to one of the supported options (`"auto"`, `"balanced"`, `"balanced_low_0"`, `"sequential"`) or create one yourself, if you want more control over where each layer should go.

  å¯é€‰çš„å‡ ç§å‚æ•°ä¸º"auto", "balanced", "balanced_low_0", "sequential", æˆ–è€…è‡ªå®šä¹‰ä¸€ä¸ª *dict* ä¹Ÿå¯

  * `auto` ä¸ `balanced` ç±»ä¼¼ï¼Œä¼šå°†æ¨¡å‹å¹³å‡åˆ’åˆ†åˆ°æ‰€æœ‰å¯ç”¨çš„GPUä¸Šï¼Œè¿™æ ·å¯ä»¥å°†batch_sizeè®¾å¤§ä¸€äº›ï¼Œæ³¨æ„å¦‚æœGPUä¸å¤Ÿï¼Œautoä¼šåŒæ—¶åˆ©ç”¨CPUå­˜å‚¨ä¸€éƒ¨åˆ†æ¨¡å‹å‚æ•°

    > The options `"auto"` and `"balanced"` produce the same results for now, but the behavior of `"auto"` might change in the future if we find a strategy that makes more sense, while `"balanced"` will stay stable.

  * `balanced_low_0`ï¼šä¼šåœ¨é™¤äº†ç¬¬ä¸€å¼ å¡ï¼ˆ"cuda:0"ï¼‰ä¹‹å¤–çš„æ‰€æœ‰GPUä¸Šå¹³å‡åˆ†é…æ¨¡å‹å‚æ•°ï¼Œè¿™æ ·å¯ä»¥åœ¨ç¬¬ä¸€å¼ å¡ä¸Šåšä¸€äº›é¢å¤–çš„æ“ä½œï¼Œä¾‹å¦‚å½“ä½¿ç”¨generateå‡½æ•°æ—¶å­˜æ”¾è¾“å‡ºæ•°æ®

  * `sequential`ï¼šå…ˆå°è¯•ç”¨ç¬¬ä¸€å¼ å¡ï¼Œå½“ç¬¬ä¸€å¼ å¡ç”¨å®Œæ—¶å¯ä»¥å†ç”¨ç¬¬äºŒå¼ å¡ï¼Œä»¥æ­¤ç±»æ¨



